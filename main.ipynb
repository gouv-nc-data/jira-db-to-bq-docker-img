{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb497b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import dlt\n",
    "from dlt.sources.sql_database import sql_database\n",
    "from loguru import logger\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('PG_URL_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9346f1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logger.remove()  # Supprimer le handler par défaut\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    format=\"<level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>\",\n",
    "    level=\"INFO\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9f71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m - \u001b[1mDébut de l'export JIRA vers BigQuery - Projet: DATA\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def load_jira_data():\n",
    "    # \"\"\"\n",
    "    # Pipeline dlt pour exporter JIRA de PostgreSQL vers BigQuery.\n",
    "    # \"\"\"\n",
    "# Configuration PostgreSQL\n",
    "pg_url_secret = os.getenv('PG_URL_SECRET', 'postgresql://user:password@ip:port/schema?options=-c%20search_path%3Dschema')\n",
    "\n",
    "# Configuration JIRA\n",
    "jira_project_key = os.getenv('JIRA_PROJECT_KEY')\n",
    "\n",
    "# Configuration BigQuery\n",
    "bq_dataset_id = os.getenv('BQ_DATASET_ID', 'jira_export')\n",
    "bq_table_id = os.getenv('BQ_TABLE_ID', 'issues')\n",
    "\n",
    "required_vars = [\n",
    "    ('PG_URL_SECRET', pg_url_secret),\n",
    "    ('JIRA_PROJECT_KEY', jira_project_key),\n",
    "    ('BQ_DATASET_ID', bq_dataset_id),\n",
    "    ('BQ_TABLE_ID', bq_table_id),\n",
    "]\n",
    "\n",
    "for var_name, var_value in required_vars:\n",
    "    if not var_value:\n",
    "        logger.error(f\"{var_name} n'est pas défini\")\n",
    "        sys.exit(1)\n",
    "\n",
    "logger.info(f\"Début de l'export JIRA vers BigQuery - Projet: {jira_project_key}\")\n",
    "\n",
    "# try:\n",
    "# Lire la requête SQL\n",
    "with open('request.sql', 'r', encoding='utf-8') as f:\n",
    "    sql_query = f.read()\n",
    "# sql_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e58c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m - \u001b[1mFichier request.sql chargé\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m - \u001b[1mInitialisation de la pipeline dlt\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m - \u001b[1mLancement de la pipeline dlt\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoine.bugeia\\pj\\jira-to-bq-cloud-run-template\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\client.py:613: UserWarning: Cannot create BigQuery Storage client, the dependency google-cloud-bigquery-storage is not installed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjira_issues\u001b[0m - \u001b[1mConnexion à la base de données pour le projet: DATA\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjira_issues\u001b[0m - \u001b[1mExécution de la requête pour le projet: DATA\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjira_issues\u001b[0m - \u001b[1mColonnes trouvées: 21\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjira_issues\u001b[0m - \u001b[1mNombre de lignes extraites: 386\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 10:44:40,424|[WARNING]|12480|28060|dlt|validate.py|verify_normalized_table:91|In schema `jira_to_bq`: The following columns in table 'issues' did not receive any data during this load and therefore could not have their types inferred:\n",
      "  - etiquettes\n",
      "  - custom_fields\n",
      "\n",
      "Unless type hints are provided, these columns will not be materialized in the destination.\n",
      "One way to provide type hints is to use the 'columns' argument in the '@dlt.resource' decorator.  For example:\n",
      "\n",
      "@dlt.resource(columns={'etiquettes': {'data_type': 'text'}})\n",
      "\n",
      "2025-11-13 10:44:40,425|[WARNING]|12480|28060|dlt|validate.py|verify_normalized_table:91|In schema `jira_to_bq`: The following columns in table 'issues__custom_fields' did not receive any data during this load and therefore could not have their types inferred:\n",
      "  - option\n",
      "\n",
      "Unless type hints are provided, these columns will not be materialized in the destination.\n",
      "One way to provide type hints is to use the 'columns' argument in the '@dlt.resource' decorator.  For example:\n",
      "\n",
      "@dlt.resource(columns={'option': {'data_type': 'text'}})\n",
      "\n",
      "c:\\Users\\antoine.bugeia\\pj\\jira-to-bq-cloud-run-template\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\client.py:613: UserWarning: Cannot create BigQuery Storage client, the dependency google-cloud-bigquery-storage is not installed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m - \u001b[1mExport terminé avec succès\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"Fichier request.sql chargé\")\n",
    "\n",
    "# Créer la pipeline dlt\n",
    "logger.info(\"Initialisation de la pipeline dlt\")\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='jira_to_bq',\n",
    "    destination=dlt.destinations.bigquery(location='EU'),\n",
    "    dataset_name=bq_dataset_id,\n",
    "    # dlt crée automatiquement le dataset s'il n'existe pas\n",
    ")\n",
    "\n",
    "# Créer la ressource avec custom SQL\n",
    "@dlt.resource(\n",
    "    table_name=bq_table_id,\n",
    "    write_disposition='replace',\n",
    "    max_table_nesting=2\n",
    ")\n",
    "def jira_issues():\n",
    "    \"\"\"Récupère les issues JIRA de PostgreSQL.\"\"\"\n",
    "    import psycopg2\n",
    "    \n",
    "    logger.info(f\"Connexion à la base de données pour le projet: {jira_project_key}\")\n",
    "    conn = psycopg2.connect(pg_url_secret)\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            logger.info(f\"Exécution de la requête pour le projet: {jira_project_key}\")\n",
    "            cursor.execute(sql_query, (jira_project_key,))\n",
    "            \n",
    "            # Récupérer les colonnes\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            logger.info(f\"Colonnes trouvées: {len(columns)}\")\n",
    "            \n",
    "            # Yielder les données\n",
    "            row_count = 0\n",
    "            for row in cursor.fetchall():\n",
    "                yield dict(zip(columns, row))\n",
    "                row_count += 1\n",
    "            \n",
    "            logger.info(f\"Nombre de lignes extraites: {row_count}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Exécuter la pipeline\n",
    "logger.info(\"Lancement de la pipeline dlt\")\n",
    "load_info = pipeline.run(jira_issues())\n",
    "# load_info = pipeline.run(jira_issues(), write_disposition=\"replace\")\n",
    "\n",
    "logger.info(\"Export terminé avec succès\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eee9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Fichier request.sql non trouvé: {e}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'export: {e}\", exc_info=True)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_jira_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e421fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf600002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jira-to-bq-cloud-run-template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
